{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Deep learning frequently requires a large amount of labeled data, but in practice, it can be very costly to collect data with labels. Semi-supervised setting has gained attention since it can leverage unlabeled data to train a model.\n",
        "\n",
        "In this tutorial, we will show you how to perform semi-supervised learning on MNIST with NNabla, using the model known as [virtual adversarial training (VAT)](https://arxiv.org/pdf/1507.00677.pdf). Although MNIST is fully labeled, we will assume a setting where some of the labels are missing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install nnabla-ext-cuda100\n",
        "!git clone https://github.com/sony/nnabla-examples.git\n",
        "%cd nnabla-examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As always, let's start by importing dependencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import nnabla as nn\n",
        "import nnabla.functions as F\n",
        "import nnabla.parametric_functions as PF\n",
        "import nnabla.solver as S\n",
        "from nnabla.logger import logger\n",
        "import nnabla.utils.save as save\n",
        "from nnabla.utils.data_iterator import data_iterator_simple\n",
        "from utils.neu.save_nnp import save_nnp\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's also define data iterator for MNIST. You can disregard the details for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "import struct\n",
        "import zlib\n",
        "\n",
        "from nnabla.logger import logger\n",
        "from nnabla.utils.data_iterator import data_iterator\n",
        "from nnabla.utils.data_source import DataSource\n",
        "from nnabla.utils.data_source_loader import download\n",
        "\n",
        "\n",
        "def load_mnist(train=True):\n",
        "    '''\n",
        "    Load MNIST dataset images and labels from the original page by Yan LeCun or the cache file.\n",
        "    Args:\n",
        "        train (bool): The testing dataset will be returned if False. Training data has 60000 images, while testing has 10000 images.\n",
        "    Returns:\n",
        "        numpy.ndarray: A shape of (#images, 1, 28, 28). Values in [0.0, 1.0].\n",
        "        numpy.ndarray: A shape of (#images, 1). Values in {0, 1, ..., 9}.\n",
        "    '''\n",
        "    if train:\n",
        "        image_uri = 'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz'\n",
        "        label_uri = 'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz'\n",
        "    else:\n",
        "        image_uri = 'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz'\n",
        "        label_uri = 'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz'\n",
        "    logger.info('Getting label data from {}.'.format(label_uri))\n",
        "    r = download(label_uri)\n",
        "    data = zlib.decompress(r.read(), zlib.MAX_WBITS | 32)\n",
        "    _, size = struct.unpack('>II', data[0:8])\n",
        "    labels = np.frombuffer(data[8:], np.uint8).reshape(-1, 1)\n",
        "    r.close()\n",
        "    logger.info('Getting label data done.')\n",
        "\n",
        "    logger.info('Getting image data from {}.'.format(image_uri))\n",
        "    r = download(image_uri)\n",
        "    data = zlib.decompress(r.read(), zlib.MAX_WBITS | 32)\n",
        "    _, size, height, width = struct.unpack('>IIII', data[0:16])\n",
        "    images = np.frombuffer(data[16:], np.uint8).reshape(\n",
        "        size, 1, height, width)\n",
        "    r.close()\n",
        "    logger.info('Getting image data done.')\n",
        "\n",
        "    return images, labels\n",
        "\n",
        "class MnistDataSource(DataSource):\n",
        "    '''\n",
        "    Get data directly from MNIST dataset from Internet(yann.lecun.com).\n",
        "    '''\n",
        "\n",
        "    def _get_data(self, position):\n",
        "        image = self._images[self._indexes[position]]\n",
        "        label = self._labels[self._indexes[position]]\n",
        "        return (image, label)\n",
        "\n",
        "    def __init__(self, train=True, shuffle=False, rng=None):\n",
        "        super(MnistDataSource, self).__init__(shuffle=shuffle)\n",
        "        self._train = train\n",
        "\n",
        "        self._images, self._labels = load_mnist(train)\n",
        "\n",
        "        self._size = self._labels.size\n",
        "        self._variables = ('x', 'y')\n",
        "        if rng is None:\n",
        "            rng = np.random.RandomState(313)\n",
        "        self.rng = rng\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        if self._shuffle:\n",
        "            self._indexes = self.rng.permutation(self._size)\n",
        "        else:\n",
        "            self._indexes = np.arange(self._size)\n",
        "        super(MnistDataSource, self).reset()\n",
        "\n",
        "    @property\n",
        "    def images(self):\n",
        "        \"\"\"Get copy of whole data with a shape of (N, 1, H, W).\"\"\"\n",
        "        return self._images.copy()\n",
        "\n",
        "    @property\n",
        "    def labels(self):\n",
        "        \"\"\"Get copy of whole label with a shape of (N, 1).\"\"\"\n",
        "        return self._labels.copy()\n",
        "\n",
        "def data_iterator_mnist(batch_size,\n",
        "                        train=True,\n",
        "                        rng=None,\n",
        "                        shuffle=True,\n",
        "                        with_memory_cache=False,\n",
        "                        with_file_cache=False):\n",
        "    '''\n",
        "    Provide DataIterator with :py:class:`MnistDataSource`\n",
        "    with_memory_cache and with_file_cache option's default value is all False,\n",
        "    because :py:class:`MnistDataSource` is able to store all data into memory.\n",
        "    For example,\n",
        "    .. code-block:: python\n",
        "        with data_iterator_mnist(True, batch_size) as di:\n",
        "            for data in di:\n",
        "                SOME CODE TO USE data.\n",
        "    '''\n",
        "    return data_iterator(MnistDataSource(train=train, shuffle=shuffle, rng=rng),\n",
        "                         batch_size,\n",
        "                         rng,\n",
        "                         with_memory_cache,\n",
        "                         with_file_cache)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now define a multi-layer perceptron (MLP) network to be used later. Our MLP consists of 3 fully-connected layers, two of whiich are followed by batch normalization and non-linear activation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "def mlp_net(x, n_h, n_y, test=False):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x(`~nnabla.Variable`): N-D array\n",
        "        n_h(int): number of units in an intermediate layer\n",
        "        n_y(int): number of classes\n",
        "        test: operation type train=True, test=False\n",
        "    Returns:\n",
        "        ~nnabla.Variable: h\n",
        "    \"\"\"\n",
        "\n",
        "    h = x\n",
        "    with nn.parameter_scope(\"fc1\"):\n",
        "        h = F.relu(PF.batch_normalization(\n",
        "            PF.affine(h, n_h), batch_stat=not test), inplace=True)\n",
        "    with nn.parameter_scope(\"fc2\"):\n",
        "        h = F.relu(PF.batch_normalization(\n",
        "            PF.affine(h, n_h), batch_stat=not test), inplace=True)\n",
        "    with nn.parameter_scope(\"fc3\"):\n",
        "        h = PF.affine(h, n_y)\n",
        "    return h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's also define a function to measure the distance between two distributions. In this example, we use a function called multinomial Kullback-Leibler divergence, commonly known as [KL-divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "def distance(y0, y1):\n",
        "    \"\"\"\n",
        "    Distance function is Kullback-Leibler Divergence for categorical distribution\n",
        "    \"\"\"\n",
        "    return F.kl_multinomial(F.softmax(y0), F.softmax(y1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we get into the main computational graph, let's also define a function to evaluate the network. This function simply returns error rate during validation, which is averaged over the number of iterations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "def calc_validation_error(di_v, xv, tv, err, val_iter):\n",
        "    \"\"\"\n",
        "    Calculate validation error rate\n",
        "    Args:\n",
        "        di_v; validation dataset\n",
        "        xv: variable for input\n",
        "        tv: variable for label\n",
        "        err: variable for error estimation\n",
        "        val_iter: number of iteration\n",
        "    Returns:\n",
        "        error rate\n",
        "    \"\"\"\n",
        "    ve = 0.0\n",
        "    for j in range(val_iter):\n",
        "        xv.d, tv.d = di_v.next()\n",
        "        xv.d = xv.d / 255\n",
        "        err.forward(clear_buffer=True)\n",
        "        ve += err.d\n",
        "    return ve / val_iter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we get into the main computational graph. We start by setting context to use cuDNN, and loading data iterator for MNIST."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "# Get context.\n",
        "from nnabla.ext_utils import get_extension_context\n",
        "ctx = get_extension_context('cudnn')\n",
        "nn.set_default_context(ctx)\n",
        "\n",
        "# Load MNIST Dataset\n",
        "images, labels = load_mnist(train=True)\n",
        "rng = np.random.RandomState(706)\n",
        "inds = rng.permutation(len(images))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's define two functions for loading data for labeled and unlabeled settings respectively. Although `feed_unlabeled` function is also returning labels, we will later see that the labels are disregarded in the graph. \n",
        "\n",
        "After declaring some hyperparameters, we also define data iterator variables using the two load functions we just defined, separately for labeled and unlabeled settings. Let's also define a data iterator variable for validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def feed_labeled(i):\n",
        "    j = inds[i]\n",
        "    return images[j], labels[j]\n",
        "\n",
        "def feed_unlabeled(i):\n",
        "    j = inds[i]\n",
        "    return images[j], labels[j]\n",
        "\n",
        "shape_x = (1, 28, 28)\n",
        "n_h = 1200 #number of units\n",
        "n_y = 10 #number of classes\n",
        "\n",
        "n_labeled = 100\n",
        "n_train = 60000\n",
        "batchsize_l = 100\n",
        "batchsize_u = 250\n",
        "batchsize_v = 100\n",
        "\n",
        "di_l = data_iterator_simple(feed_labeled, n_labeled,\n",
        "                            batchsize_l, shuffle=True, rng=rng, with_file_cache=False)\n",
        "di_u = data_iterator_simple(feed_unlabeled, n_train,\n",
        "                            batchsize_u, shuffle=True, rng=rng, with_file_cache=False)\n",
        "di_v = data_iterator_mnist(batchsize_v, train=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first define a simple forward function that calls the multi-layer perceptron network that we defined above.\n",
        "\n",
        "We then define the variables separately for labeled and unlabeled data. `xl`, `xu` and `yl`,`yu` refer to input and output for MLP network. In the labeled setting, we also have teacher variable `tl`, from which we can calculate the loss by applying softmax cross entropy. Note that this loss is for labeled data only and we will define separate loss variable later for unlabeled data.\n",
        "\n",
        "Also, notice that we do not have teacher variable for unlabeled setting, because we assume that the labels are missing. Instead, we define an unlinked variable of `yu`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create networks\n",
        "# feed-forward-net building function\n",
        "def forward(x, test=False):\n",
        "    return mlp_net(x, n_h, n_y, test)\n",
        "\n",
        "# Net for learning labeled data\n",
        "xl = nn.Variable((batchsize_l,) + shape_x, need_grad=False)\n",
        "yl = forward(xl, test=False)\n",
        "tl = nn.Variable((batchsize_l, 1), need_grad=False)\n",
        "loss_l = F.mean(F.softmax_cross_entropy(yl, tl))\n",
        "\n",
        "# Net for learning unlabeled data\n",
        "xu = nn.Variable((batchsize_u,) + shape_x, need_grad=False)\n",
        "yu = forward(xu, test=False)\n",
        "y1 = yu.get_unlinked_variable()\n",
        "y1.need_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now define variables for noise, which are added to the input variable xu and fed to MLP. The KL-divergence between the MLP outputs of noisy variable and noise-free variable is used to compute loss. Of the two losses, one is used to perform power method iteration, and another one is loss for unlabeled data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "xi_for_vat = 10.0\n",
        "eps_for_vat = 1.5\n",
        "\n",
        "noise = nn.Variable((batchsize_u,) + shape_x, need_grad=True)\n",
        "r = noise / (F.sum(noise ** 2, [1, 2, 3], keepdims=True)) ** 0.5\n",
        "r.persistent = True\n",
        "y2 = forward(xu + xi_for_vat * r, test=False)\n",
        "y3 = forward(xu + eps_for_vat * r, test=False)\n",
        "loss_k = F.mean(distance(y1, y2))\n",
        "loss_u = F.mean(distance(y1, y3))\n",
        "\n",
        "# Net for evaluating validation data\n",
        "xv = nn.Variable((batchsize_v,) + shape_x, need_grad=False)\n",
        "hv = forward(xv, test=True)\n",
        "tv = nn.Variable((batchsize_v, 1), need_grad=False)\n",
        "err = F.mean(F.top_n_error(hv, tv, n=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define our solver and monitor variables. We will use [Adam](https://arxiv.org/pdf/1412.6980.pdf) as our solver."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create solver\n",
        "solver = S.Adam(2e-3)\n",
        "solver.set_parameters(nn.get_parameters())\n",
        "\n",
        "# Monitor training and validation stats.\n",
        "model_save_path = 'tmp.monitor.vat'\n",
        "import nnabla.monitor as M\n",
        "monitor = M.Monitor(model_save_path)\n",
        "monitor_verr = M.MonitorSeries(\"Test error\", monitor, interval=240)\n",
        "monitor_time = M.MonitorTimeElapsed(\"Elapsed time\", monitor, interval=240)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we get into our training loop. We will have separate training stages for labeled and unlabeled data. We first start with labeled data, which is pretty much the same as usual training graph.\n",
        "\n",
        "Then, we define our training graph for unlabeled data. Note that we are ignoring the label returned by data iterator, setting it as a garbage variable `_`. We first forward the noise-free variable, and then calculate adversarial noise first by generating random noise followed by power method over iterations. Finally, we compute loss for unlabeled data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Training Loop.\n",
        "t0 = time.time()\n",
        "max_iter = 24000\n",
        "val_interval = 240\n",
        "val_iter = 100\n",
        "weight_decay = 0\n",
        "n_iter_for_power_method = 1\n",
        "iter_per_epoch = 240\n",
        "learning_rate_decay = 0.9\n",
        "\n",
        "for i in range(max_iter):\n",
        "\n",
        "    # Validation Test\n",
        "    if i % val_interval == 0:\n",
        "        valid_error = calc_validation_error(\n",
        "            di_v, xv, tv, err, val_iter)\n",
        "        monitor_verr.add(i, valid_error)\n",
        "\n",
        "    #################################\n",
        "    ## Training by Labeled Data #####\n",
        "    #################################\n",
        "\n",
        "    # forward, backward and update\n",
        "    xl.d, tl.d = di_l.next()\n",
        "    xl.d = xl.d / 255\n",
        "    solver.zero_grad()\n",
        "    loss_l.forward(clear_no_need_grad=True)\n",
        "    loss_l.backward(clear_buffer=True)\n",
        "    solver.weight_decay(weight_decay)\n",
        "    solver.update()\n",
        "\n",
        "\n",
        "    #################################\n",
        "    ## Training by Unlabeled Data ###\n",
        "    #################################\n",
        "\n",
        "    # Calculate y without noise, only once.\n",
        "    xu.d, _ = di_u.next()\n",
        "    xu.d = xu.d / 255\n",
        "    yu.forward(clear_buffer=True)\n",
        "\n",
        "    ##### Calculate Adversarial Noise #####\n",
        "    # Do power method iteration\n",
        "    noise.d = np.random.normal(size=xu.shape).astype(np.float32)\n",
        "    for k in range(n_iter_for_power_method):\n",
        "        r.grad.zero()\n",
        "        loss_k.forward(clear_no_need_grad=True)\n",
        "        loss_k.backward(clear_buffer=True)\n",
        "        noise.data.copy_from(r.grad)\n",
        "\n",
        "    ##### Calculate loss for unlabeled data #####\n",
        "    # forward, backward and update\n",
        "    solver.zero_grad()\n",
        "    loss_u.forward(clear_no_need_grad=True)\n",
        "    loss_u.backward(clear_buffer=True)\n",
        "    solver.weight_decay(weight_decay)\n",
        "    solver.update()\n",
        "\n",
        "    ##### Learning rate update #####\n",
        "    if i % iter_per_epoch == 0:\n",
        "        solver.set_learning_rate(\n",
        "            solver.learning_rate() * learning_rate_decay)\n",
        "    monitor_time.add(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we evaluate our model on the validation dataset. If the model was trained correctly, we should get an error rate of around 1.5%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "# Evaluate the final model by the error rate with validation dataset\n",
        "valid_error = calc_validation_error(di_v, xv, tv, err, val_iter)\n",
        "print(valid_error)\n",
        "\n",
        "# If you need to save the model, please comment out the following lines:\n",
        "# parameter_file = os.path.join(\n",
        "#     model_save_path, 'params_%06d.h5' % max_iter)\n",
        "# nn.save_parameters(parameter_file)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "accelerator": "GPU", 
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
