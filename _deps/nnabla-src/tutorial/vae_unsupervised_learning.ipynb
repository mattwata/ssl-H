{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this tutorial, we will show you how to implement [variational auto-encoder (VAE)](https://arxiv.org/pdf/1312.6114.pdf) in NNabla, which can be a powerful model for unsupervised learning by estimating variational lower bound. VAE can also be extended to many applications, such as image generation, as demonstrated by [VQ-VAE](https://arxiv.org/pdf/1711.00937.pdf).\n",
        "\n",
        "We will use MNIST for our tutorial. Although MNIST is fully labeled, we will assume a setting in which the labels are missing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install nnabla-ext-cuda100\n",
        "!git clone https://github.com/sony/nnabla-examples.git\n",
        "%cd nnabla-examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As always, let's start by importing dependencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import nnabla as nn\n",
        "import nnabla.functions as F\n",
        "import nnabla.parametric_functions as PF\n",
        "import nnabla.monitor as M\n",
        "import nnabla.solver as S\n",
        "from nnabla.logger import logger\n",
        "from utils.neu.save_nnp import save_nnp\n",
        "\n",
        "import nnabla.utils.save as save\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's also define data iterator for MNIST. You can disregard the details for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "import struct\n",
        "import zlib\n",
        "\n",
        "from nnabla.logger import logger\n",
        "from nnabla.utils.data_iterator import data_iterator\n",
        "from nnabla.utils.data_source import DataSource\n",
        "from nnabla.utils.data_source_loader import download\n",
        "\n",
        "\n",
        "def load_mnist(train=True):\n",
        "    '''\n",
        "    Load MNIST dataset images and labels from the original page by Yan LeCun or the cache file.\n",
        "    Args:\n",
        "        train (bool): The testing dataset will be returned if False. Training data has 60000 images, while testing has 10000 images.\n",
        "    Returns:\n",
        "        numpy.ndarray: A shape of (#images, 1, 28, 28). Values in [0.0, 1.0].\n",
        "        numpy.ndarray: A shape of (#images, 1). Values in {0, 1, ..., 9}.\n",
        "    '''\n",
        "    if train:\n",
        "        image_uri = 'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz'\n",
        "        label_uri = 'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz'\n",
        "    else:\n",
        "        image_uri = 'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz'\n",
        "        label_uri = 'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz'\n",
        "    logger.info('Getting label data from {}.'.format(label_uri))\n",
        "    r = download(label_uri)\n",
        "    data = zlib.decompress(r.read(), zlib.MAX_WBITS | 32)\n",
        "    _, size = struct.unpack('>II', data[0:8])\n",
        "    labels = np.frombuffer(data[8:], np.uint8).reshape(-1, 1)\n",
        "    r.close()\n",
        "    logger.info('Getting label data done.')\n",
        "\n",
        "    logger.info('Getting image data from {}.'.format(image_uri))\n",
        "    r = download(image_uri)\n",
        "    data = zlib.decompress(r.read(), zlib.MAX_WBITS | 32)\n",
        "    _, size, height, width = struct.unpack('>IIII', data[0:16])\n",
        "    images = np.frombuffer(data[16:], np.uint8).reshape(\n",
        "        size, 1, height, width)\n",
        "    r.close()\n",
        "    logger.info('Getting image data done.')\n",
        "\n",
        "    return images, labels\n",
        "\n",
        "class MnistDataSource(DataSource):\n",
        "    '''\n",
        "    Get data directly from MNIST dataset from Internet(yann.lecun.com).\n",
        "    '''\n",
        "\n",
        "    def _get_data(self, position):\n",
        "        image = self._images[self._indexes[position]]\n",
        "        label = self._labels[self._indexes[position]]\n",
        "        return (image, label)\n",
        "\n",
        "    def __init__(self, train=True, shuffle=False, rng=None):\n",
        "        super(MnistDataSource, self).__init__(shuffle=shuffle)\n",
        "        self._train = train\n",
        "\n",
        "        self._images, self._labels = load_mnist(train)\n",
        "\n",
        "        self._size = self._labels.size\n",
        "        self._variables = ('x', 'y')\n",
        "        if rng is None:\n",
        "            rng = np.random.RandomState(313)\n",
        "        self.rng = rng\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        if self._shuffle:\n",
        "            self._indexes = self.rng.permutation(self._size)\n",
        "        else:\n",
        "            self._indexes = np.arange(self._size)\n",
        "        super(MnistDataSource, self).reset()\n",
        "\n",
        "    @property\n",
        "    def images(self):\n",
        "        \"\"\"Get copy of whole data with a shape of (N, 1, H, W).\"\"\"\n",
        "        return self._images.copy()\n",
        "\n",
        "    @property\n",
        "    def labels(self):\n",
        "        \"\"\"Get copy of whole label with a shape of (N, 1).\"\"\"\n",
        "        return self._labels.copy()\n",
        "\n",
        "def data_iterator_mnist(batch_size,\n",
        "                        train=True,\n",
        "                        rng=None,\n",
        "                        shuffle=True,\n",
        "                        with_memory_cache=False,\n",
        "                        with_file_cache=False):\n",
        "    '''\n",
        "    Provide DataIterator with :py:class:`MnistDataSource`\n",
        "    with_memory_cache and with_file_cache option's default value is all False,\n",
        "    because :py:class:`MnistDataSource` is able to store all data into memory.\n",
        "    For example,\n",
        "    .. code-block:: python\n",
        "        with data_iterator_mnist(True, batch_size) as di:\n",
        "            for data in di:\n",
        "                SOME CODE TO USE data.\n",
        "    '''\n",
        "    return data_iterator(MnistDataSource(train=train, shuffle=shuffle, rng=rng),\n",
        "                         batch_size,\n",
        "                         rng,\n",
        "                         with_memory_cache,\n",
        "                         with_file_cache)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now define a function for our VAE network, which is the most essential part of this tutorial. This function will calculate Elbo (evidence lower-bound) loss. Note that this sample employs a Bernoulli generator version.\n",
        "\n",
        "**Encoder:**\n",
        "\n",
        "We first define the encoder network. We first normalize the input, and stack two fully-connected (affine) layers, each of which is followed by ELU non-linear activation function (the original paper uses Softplus activation function). We can then calculate `mu` and `sigma` from the output of two-stack affine layers, which are used to compute `z`. Note that during training, random noise is introduced to `z`, whereas it is set to be equal to `mu` for test.\n",
        "\n",
        "**Decoder:**\n",
        "\n",
        "We now define our decoder network, where we also stack two fully-connected layers followed by ELU, but this time the input is `z` computed from the encoder network. By applying another fully-connected layer to the output of two-stack fully-connected layers, we have the Bernoulli probabilities for each pixel.\n",
        "\n",
        "**Elbo Loss:**\n",
        "\n",
        "We first binarize the normalized input. Then we estimate the expectations over 3 likelihoods, namely `q(z|x)`, `p(z)`, and `p(x|z)`. Finally, from the expectation estimates, we can compute VAE loss, by computing the negative evidence lower bound.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "def vae(x, shape_z, test=False):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x(`~nnabla.Variable`): N-D array\n",
        "        shape_z(tuple of int): size of z\n",
        "        test : True=train, False=test\n",
        "    Returns:\n",
        "        ~nnabla.Variable: Elbo loss\n",
        "    \"\"\"\n",
        "\n",
        "    #############################################\n",
        "    # Encoder of 2 fully connected layers       #\n",
        "    #############################################\n",
        "\n",
        "    # Normalize input\n",
        "    xa = x / 256.\n",
        "    batch_size = x.shape[0]\n",
        "\n",
        "    # 2 fully connected layers, and Elu replaced from original Softplus.\n",
        "    h = F.elu(PF.affine(xa, (500,), name='fc1'))\n",
        "    h = F.elu(PF.affine(h, (500,), name='fc2'))\n",
        "\n",
        "    # The outputs are the parameters of Gauss probability density.\n",
        "    mu = PF.affine(h, shape_z, name='fc_mu')\n",
        "    logvar = PF.affine(h, shape_z, name='fc_logvar')\n",
        "    sigma = F.exp(0.5 * logvar)\n",
        "\n",
        "    # The prior variable and the reparameterization trick\n",
        "    if not test:\n",
        "        # training with reparameterization trick\n",
        "        epsilon = F.randn(mu=0, sigma=1, shape=(batch_size,) + shape_z)\n",
        "        z = mu + sigma * epsilon\n",
        "    else:\n",
        "        # test without randomness\n",
        "        z = mu\n",
        "\n",
        "    #############################################\n",
        "    # Decoder of 2 fully connected layers       #\n",
        "    #############################################\n",
        "\n",
        "    # 2 fully connected layers, and Elu replaced from original Softplus.\n",
        "    h = F.elu(PF.affine(z, (500,), name='fc3'))\n",
        "    h = F.elu(PF.affine(h, (500,), name='fc4'))\n",
        "\n",
        "    # The outputs are the parameters of Bernoulli probabilities for each pixel.\n",
        "    prob = PF.affine(h, (1, 28, 28), name='fc5')\n",
        "\n",
        "    #############################################\n",
        "    # Elbo components and loss objective        #\n",
        "    #############################################\n",
        "\n",
        "    # Binarized input\n",
        "    xb = F.greater_equal_scalar(xa, 0.5)\n",
        "\n",
        "    # E_q(z|x)[log(q(z|x))]\n",
        "    # without some constant terms that will be canceled after summation of loss\n",
        "    logqz = 0.5 * F.sum(1.0 + logvar, axis=1)\n",
        "\n",
        "    # E_q(z|x)[log(p(z))]\n",
        "    # without some constant terms that will be canceled after summation of loss\n",
        "    logpz = 0.5 * F.sum(mu * mu + sigma * sigma, axis=1)\n",
        "\n",
        "    # E_q(z|x)[log(p(x|z))]\n",
        "    logpx = F.sum(F.sigmoid_cross_entropy(prob, xb), axis=(1, 2, 3))\n",
        "\n",
        "    # Vae loss, the negative evidence lowerbound\n",
        "    loss = F.mean(logpx + logpz - logqz)\n",
        "\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we start training, let's set our context to use GPU and define data iterator variables for training and test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from nnabla.ext_utils import get_extension_context\n",
        "ctx = get_extension_context('cudnn')\n",
        "nn.set_default_context(ctx)\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "# Initialize data provider\n",
        "di_l = data_iterator_mnist(batch_size, True)\n",
        "di_t = data_iterator_mnist(batch_size, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now define the input variable and loss variables for our graph. We define two losses for training and test respectively, which are computed from the VAE network we defined above.\n",
        "\n",
        "Let's also set solver and monitor variables to track the progress of training. We use [Adam](https://arxiv.org/pdf/1412.6980.pdf) as our solver (which by the way is from the same author as VAE!)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Network\n",
        "shape_x = (1, 28, 28)\n",
        "shape_z = (50,)\n",
        "x = nn.Variable((batch_size,) + shape_x)\n",
        "loss_l = vae(x, shape_z, test=False)\n",
        "loss_t = vae(x, shape_z, test=True)\n",
        "\n",
        "# Create solver\n",
        "learning_rate = 3e-4\n",
        "solver = S.Adam(learning_rate)\n",
        "solver.set_parameters(nn.get_parameters())\n",
        "\n",
        "# Monitors for training and validation\n",
        "model_save_path = 'tmp.monitor.vae'\n",
        "monitor = M.Monitor(model_save_path)\n",
        "monitor_training_loss = M.MonitorSeries(\n",
        "    \"Training loss\", monitor, interval=600)\n",
        "monitor_test_loss = M.MonitorSeries(\"Test loss\", monitor, interval=600)\n",
        "monitor_time = M.MonitorTimeElapsed(\"Elapsed time\", monitor, interval=600)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now perform training, where we iterateively retrieve the data, compute loss from VAE network, and update the solver. Note that the labels provided by the data iterators are disregarded by setting garbage variables `_`. \n",
        "\n",
        "By the end of the training, you will have test loss of approximately 85."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Training Loop.\n",
        "max_iter = 60000\n",
        "weight_decay=0\n",
        "for i in range(max_iter):\n",
        "\n",
        "    # Initialize gradients\n",
        "    solver.zero_grad()\n",
        "\n",
        "    # Forward, backward and update\n",
        "    x.d, _ = di_l.next()\n",
        "    loss_l.forward(clear_no_need_grad=True)\n",
        "    loss_l.backward(clear_buffer=True)\n",
        "    solver.weight_decay(weight_decay)\n",
        "    solver.update()\n",
        "\n",
        "    # Forward for test\n",
        "    x.d, _ = di_t.next()\n",
        "    loss_t.forward(clear_no_need_grad=True)\n",
        "\n",
        "    # Monitor for logging\n",
        "    monitor_training_loss.add(i, loss_l.d.copy())\n",
        "    monitor_test_loss.add(i, loss_t.d.copy())\n",
        "    monitor_time.add(i)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "accelerator": "GPU", 
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
