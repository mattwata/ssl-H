{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **NNabla Cifar-10 Training Tutorial**\n",
        "  In this tutorial, we show how to train a classifier on Cifar-10 dataset using nnabla, including setting up data-iterator and network.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install nnabla-ext-cuda100\n",
        "!git clone https://github.com/sony/nnabla-examples.git\n",
        "%cd nnabla-examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's import dependencies first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys\n",
        "import time\n",
        "import nnabla as nn\n",
        "from nnabla.ext_utils import get_extension_context\n",
        "import nnabla.functions as F\n",
        "import nnabla.parametric_functions as PF\n",
        "import nnabla.solvers as S\n",
        "import numpy as np\n",
        "import functools\n",
        "import nnabla.utils.save as save\n",
        "\n",
        "from utils.neu.checkpoint_util import save_checkpoint, load_checkpoint\n",
        "from utils.neu.save_nnp import save_nnp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then define a data iterator for Cifar-10. When called, it'll also download the dataset, and pass the samples to the network during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from contextlib import contextmanager\n",
        "import struct\n",
        "import tarfile\n",
        "import zlib\n",
        "import errno\n",
        "\n",
        "from nnabla.logger import logger\n",
        "from nnabla.utils.data_iterator import data_iterator\n",
        "from nnabla.utils.data_source import DataSource\n",
        "from nnabla.utils.data_source_loader import download, get_data_home\n",
        "\n",
        "\n",
        "class Cifar10DataSource(DataSource):\n",
        "    '''\n",
        "    Get data directly from cifar10 dataset from Internet(yann.lecun.com).\n",
        "    '''\n",
        "\n",
        "    def _get_data(self, position):\n",
        "        image = self._images[self._indexes[position]]\n",
        "        label = self._labels[self._indexes[position]]\n",
        "        return (image, label)\n",
        "\n",
        "\n",
        "    def __init__(self, train=True, shuffle=False, rng=None):\n",
        "        super(Cifar10DataSource, self).__init__(shuffle=shuffle, rng=rng)\n",
        "        self._train = train\n",
        "        data_uri = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
        "        logger.info('Getting labeled data from {}.'.format(data_uri))\n",
        "        r = download(data_uri)  # file object returned\n",
        "        with tarfile.open(fileobj=r, mode=\"r:gz\") as fpin:\n",
        "            # Training data\n",
        "            if train:\n",
        "                images = []\n",
        "                labels = []\n",
        "                for member in fpin.getmembers():\n",
        "                    if \"data_batch\" not in member.name:\n",
        "                        continue\n",
        "                    fp = fpin.extractfile(member)\n",
        "                    data = np.load(fp, encoding=\"bytes\", allow_pickle=True)\n",
        "                    images.append(data[b\"data\"])\n",
        "                    labels.append(data[b\"labels\"])\n",
        "                self._size = 50000\n",
        "                self._images = np.concatenate(\n",
        "                    images).reshape(self._size, 3, 32, 32)\n",
        "                self._labels = np.concatenate(labels).reshape(-1, 1)\n",
        "            # Validation data\n",
        "            else:\n",
        "                for member in fpin.getmembers():\n",
        "                    if \"test_batch\" not in member.name:\n",
        "                        continue\n",
        "                    fp = fpin.extractfile(member)\n",
        "                    data = np.load(fp, encoding=\"bytes\", allow_pickle=True)\n",
        "                    images = data[b\"data\"]\n",
        "                    labels = data[b\"labels\"]\n",
        "                self._size = 10000\n",
        "                self._images = images.reshape(self._size, 3, 32, 32)\n",
        "                self._labels = np.array(labels).reshape(-1, 1)\n",
        "        r.close()\n",
        "        logger.info('Getting labeled data from {}.'.format(data_uri))\n",
        "\n",
        "        self._size = self._labels.size\n",
        "        self._variables = ('x', 'y')\n",
        "        if rng is None:\n",
        "            rng = np.random.RandomState(313)\n",
        "        self.rng = rng\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        if self._shuffle:\n",
        "            self._indexes = self.rng.permutation(self._size)\n",
        "        else:\n",
        "            self._indexes = np.arange(self._size)\n",
        "        super(Cifar10DataSource, self).reset()\n",
        "\n",
        "    @property\n",
        "    def images(self):\n",
        "        \"\"\"Get copy of whole data with a shape of (N, 1, H, W).\"\"\"\n",
        "        return self._images.copy()\n",
        "\n",
        "    @property\n",
        "    def labels(self):\n",
        "        \"\"\"Get copy of whole label with a shape of (N, 1).\"\"\"\n",
        "        return self._labels.copy()\n",
        "\n",
        "\n",
        "def data_iterator_cifar10(batch_size,\n",
        "                          train=True,\n",
        "                          rng=None,\n",
        "                          shuffle=True,\n",
        "                          with_memory_cache=False,\n",
        "                          with_file_cache=False):\n",
        "    '''\n",
        "    Provide DataIterator with :py:class:`Cifar10DataSource`\n",
        "    with_memory_cache and with_file_cache option's default value is all False,\n",
        "    because :py:class:`Cifar10DataSource` is able to store all data into memory.\n",
        "\n",
        "    '''\n",
        "    return data_iterator(Cifar10DataSource(train=train, shuffle=shuffle, rng=rng),\n",
        "                         batch_size,\n",
        "                         rng,\n",
        "                         with_memory_cache,\n",
        "                         with_file_cache)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def categorical_error(pred, label):\n",
        "    pred_label = pred.argmax(1)\n",
        "    return (pred_label != label.flat).mean()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now define our neural network. In this example, we employ a slightly modified architecture based on [ResNet](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf). We are also performing data augmentation here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def resnet23_prediction(image, test=False, ncls=10, nmaps=64, act=F.relu):\n",
        "    \"\"\"\n",
        "    Construct ResNet 23\n",
        "    \"\"\"\n",
        "    # Residual Unit\n",
        "    def res_unit(x, scope_name, dn=False):\n",
        "        C = x.shape[1]\n",
        "        with nn.parameter_scope(scope_name):\n",
        "            # Conv -> BN -> Nonlinear\n",
        "            with nn.parameter_scope(\"conv1\"):\n",
        "                h = PF.convolution(x, C // 2, kernel=(1, 1), pad=(0, 0),\n",
        "                                   with_bias=False)\n",
        "                h = PF.batch_normalization(h, batch_stat=not test)\n",
        "                h = act(h)\n",
        "            # Conv -> BN -> Nonlinear\n",
        "            with nn.parameter_scope(\"conv2\"):\n",
        "                h = PF.convolution(h, C // 2, kernel=(3, 3), pad=(1, 1),\n",
        "                                   with_bias=False)\n",
        "                h = PF.batch_normalization(h, batch_stat=not test)\n",
        "                h = act(h)\n",
        "            # Conv -> BN\n",
        "            with nn.parameter_scope(\"conv3\"):\n",
        "                h = PF.convolution(h, C, kernel=(1, 1), pad=(0, 0),\n",
        "                                   with_bias=False)\n",
        "                h = PF.batch_normalization(h, batch_stat=not test)\n",
        "            # Residual -> Nonlinear\n",
        "            h = act(F.add2(h, x, inplace=True))\n",
        "            # Maxpooling\n",
        "            if dn:\n",
        "                h = F.max_pooling(h, kernel=(2, 2), stride=(2, 2))\n",
        "            return h\n",
        "    # Conv -> BN -> Nonlinear\n",
        "    with nn.parameter_scope(\"conv1\"):\n",
        "        # Preprocess\n",
        "        if not test:\n",
        "            image = F.image_augmentation(image, contrast=1.0,\n",
        "                                         angle=0.25,\n",
        "                                         flip_lr=True)\n",
        "            image.need_grad = False\n",
        "        h = PF.convolution(image, nmaps, kernel=(3, 3),\n",
        "                           pad=(1, 1), with_bias=False)\n",
        "        h = PF.batch_normalization(h, batch_stat=not test)\n",
        "        h = act(h)\n",
        "\n",
        "    h = res_unit(h, \"conv2\", False)    # -> 32x32\n",
        "    h = res_unit(h, \"conv3\", True)     # -> 16x16\n",
        "    h = res_unit(h, \"conv4\", False)    # -> 16x16\n",
        "    h = res_unit(h, \"conv5\", True)     # -> 8x8\n",
        "    h = res_unit(h, \"conv6\", False)    # -> 8x8\n",
        "    h = res_unit(h, \"conv7\", True)     # -> 4x4\n",
        "    h = res_unit(h, \"conv8\", False)    # -> 4x4\n",
        "    h = F.average_pooling(h, kernel=(4, 4))  # -> 1x1\n",
        "    pred = PF.affine(h, ncls)\n",
        "\n",
        "    return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define our loss function, which in this case is the mean of softmax cross entropy, computed from the predictions and the labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def loss_function(pred, label):\n",
        "    loss = F.mean(F.softmax_cross_entropy(pred, label))\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are almost ready to start training! Let's define some hyper-parameters for the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "n_train_samples = 50000\n",
        "batch_size = 64\n",
        "bs_valid = 64 #batch size for validation\n",
        "extension_module = 'cudnn'\n",
        "ctx = get_extension_context(\n",
        "    extension_module)\n",
        "nn.set_default_context(ctx)\n",
        "prediction = functools.partial(\n",
        "    resnet23_prediction, ncls=10, nmaps=64, act=F.relu)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then create our training and validation graphs. Note that labels are not provided for validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create training graphs\n",
        "test = False\n",
        "image_train = nn.Variable((batch_size, 3, 32, 32))\n",
        "label_train = nn.Variable((batch_size, 1))\n",
        "pred_train = prediction(image_train, test)\n",
        "loss_train = loss_function(pred_train, label_train)\n",
        "input_image_train = {\"image\": image_train, \"label\": label_train}\n",
        "\n",
        "# Create validation graph\n",
        "test = True\n",
        "image_valid = nn.Variable((bs_valid, 3, 32, 32))\n",
        "pred_valid = prediction(image_valid, test)\n",
        "input_image_valid = {\"image\": image_valid}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's also define our solver. We employ [Adam](https://arxiv.org/pdf/1412.6980.pdf) in this example, but other solvers can be used too. Let's also define monitor variables to keep track of the progress during training. Note that, if you want to load previously saved weight parameters, you can load it using `load_checkpoint`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Solvers\n",
        "solver = S.Adam()\n",
        "solver.set_parameters(nn.get_parameters())\n",
        "start_point = 0\n",
        "\n",
        "# If necessary, load weights and solver state info from specified checkpoint file.\n",
        "# start_point = load_checkpoint(specified_checkpoint, solver)\n",
        "\n",
        "# Create monitor\n",
        "from nnabla.monitor import Monitor, MonitorSeries, MonitorTimeElapsed\n",
        "monitor = Monitor('tmp.monitor')\n",
        "monitor_loss = MonitorSeries(\"Training loss\", monitor, interval=10)\n",
        "monitor_err = MonitorSeries(\"Training error\", monitor, interval=10)\n",
        "monitor_time = MonitorTimeElapsed(\"Training time\", monitor, interval=10)\n",
        "monitor_verr = MonitorSeries(\"Test error\", monitor, interval=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define data iterator variables separately for training and validation, using the data iterator we defined earlier. Note that the second argument is different for each variable, depending whether it is for training or validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Data Iterator\n",
        "tdata = data_iterator_cifar10(batch_size, True)\n",
        "vdata = data_iterator_cifar10(batch_size, False)\n",
        "\n",
        "# save intermediate weights if you need\n",
        "#contents = save_nnp({'x': image_valid}, {'y': pred_valid}, batch_size)\n",
        "#save.save(os.path.join('tmp.monitor',\n",
        "#                       '{}_epoch0_result.nnp'.format('cifar10_resnet23')), contents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are good to go now! Start training, get a coffee, and watch how the training loss and test error decline as the training proceeds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "max_iter = 40000\n",
        "val_iter = 100\n",
        "model_save_interval = 10000\n",
        "model_save_path = 'tmp.monitor'\n",
        "# Training-loop\n",
        "for i in range(start_point, max_iter):\n",
        "    # Validation\n",
        "    if i % int(n_train_samples / batch_size) == 0:\n",
        "        ve = 0.\n",
        "        for j in range(val_iter):\n",
        "            image, label = vdata.next()\n",
        "            input_image_valid[\"image\"].d = image\n",
        "            pred_valid.forward()\n",
        "            ve += categorical_error(pred_valid.d, label)\n",
        "        ve /= val_iter\n",
        "        monitor_verr.add(i, ve)\n",
        "    if int(i % model_save_interval) == 0:\n",
        "        # save checkpoint file\n",
        "        save_checkpoint(model_save_path, i, solver)\n",
        "\n",
        "    # Forward/Zerograd/Backward\n",
        "    image, label = tdata.next()\n",
        "    input_image_train[\"image\"].d = image\n",
        "    input_image_train[\"label\"].d = label\n",
        "    loss_train.forward()\n",
        "    solver.zero_grad()\n",
        "    loss_train.backward()\n",
        "\n",
        "    # Solvers update\n",
        "    solver.update()\n",
        "\n",
        "    e = categorical_error(\n",
        "        pred_train.d, input_image_train[\"label\"].d)\n",
        "    monitor_loss.add(i, loss_train.d.copy())\n",
        "    monitor_err.add(i, e)\n",
        "    monitor_time.add(i)\n",
        "\n",
        "nn.save_parameters(os.path.join(model_save_path,\n",
        "                                'params_%06d.h5' % (max_iter)))\n",
        "\n",
        "# save_nnp_lastepoch\n",
        "contents = save_nnp({'x': image_valid}, {'y': pred_valid}, batch_size)\n",
        "save.save(os.path.join(model_save_path,\n",
        "                       '{}_result.nnp'.format('cifar10_resnet23')), contents)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "accelerator": "GPU", 
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
